{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJNsvSFac3K"
   },
   "source": [
    "# Train CNN Classifier on human_ocr_ensembl dataset\n",
    "\n",
    "The dataset comes from the [Genomic Benchmarks](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks). Best reaults achieved are reported in these [tables](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/tree/main/experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from genomic_benchmarks.data_check import info\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `human_enhancers_cohn` has 2 classes: negative, positive.\n",
      "\n",
      "All lengths of genomic intervals equals 500.\n",
      "\n",
      "Totally 27791 sequences have been found, 20843 for training and 6948 for testing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>10422</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>10421</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train  test\n",
       "negative  10422  3474\n",
       "positive  10421  3474"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(\"human_enhancers_cohn\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"katarinagresova/Genomic_Benchmarks_human_enhancers_cohn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq': 'TGGTGGTACTTGTCAGGACTTGGAGCAGCAGGTGCAAGATTTAGTGGGTTGGTTTTAGAATATCTGCTTGGAAAGTGGAAAAACTCAATGGATCATCTAGACTTTGGAATTTATCTCCTTCCCCACTTCTCCACTCCCCCAACAACAACAACAACAACAATGACAACAAAAACACCTGGAATAAACAGGTCATACAACGAGGTAGTTGATAGAATAATGTACTTTCCTTTCAGGCACCCCTTGGAGGAGGCAGATTCTGCCCTTTAAGCTGAATCTGCCTTTCCTGCATTTCCTGAAACTCCTGCATTTCCTGAAATCTTCCTGTATTTTCCTGAAATTTCCTGCCATTCCTGAAACTTTAAGGTAACTGTGTCATTAAAGGAAGGAGAGAAGGGAAGTATTAGGACTGCAGATTTGGGGTGCATGATCAGCCTGGCTCTGAGCTTGCAGACTCCCAGAGTCAGGGAAGGGAGGAGCCACCAGCAACCTTGTGGCTTACT',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, max_length=500):\n",
    "    one_hot = torch.zeros((4, max_length), dtype=torch.float32)\n",
    "    \n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    \n",
    "    for i, nucleotide in enumerate(sequence[:max_length]):\n",
    "        if nucleotide in mapping:\n",
    "            one_hot[mapping[nucleotide], i] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "    \n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.dataset[idx]['seq']\n",
    "        label = self.dataset[idx]['label']\n",
    "        encoded_seq = one_hot_encode(seq)\n",
    "        return encoded_seq, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset[\"train\"].with_format(\"torch\")\n",
    "ds = DNADataset(ds)\n",
    "\n",
    "train_size = int(0.8 * len(ds))\n",
    "val_size = len(ds) - train_size\n",
    "\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN for binary classification of DNA sequences\n",
    "class DNAClassifierCNN(nn.Module):\n",
    "    def __init__(self, kernel_size=4):\n",
    "        super(DNAClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=kernel_size, stride=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=kernel_size, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(self.count_flatten_size(), 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def count_flatten_size(self):\n",
    "        dummy_input = torch.zeros(1, 4, 500)\n",
    "        dummy_output = self.pool(self.conv2(self.bn1(self.pool(self.conv1(dummy_input)))))\n",
    "        return dummy_output.view(-1).size(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs.to(DEVICE))\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            labels = labels.float().to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = (outputs.squeeze() > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model training and evaluation after each epoch\n",
    "def evaluation_loop(model, epochs, lr):\n",
    "    \n",
    "    adam = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, train_loader, adam, criterion)\n",
    "        avg_loss, accuracy = evaluate_model(model, val_loader, criterion)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    avg_loss, accuracy = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "    print(f'Loss: {avg_loss}, Accuracy: {accuracy}\\n')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Validation Loss: 0.5858050443743932, Accuracy: 0.6792995922283521\n",
      "Epoch 2/6, Validation Loss: 0.5629535205946624, Accuracy: 0.7020868313744303\n",
      "Epoch 3/6, Validation Loss: 0.5631931863213313, Accuracy: 0.7004077716478772\n",
      "Epoch 4/6, Validation Loss: 0.552603553724653, Accuracy: 0.7114415927080835\n",
      "Epoch 5/6, Validation Loss: 0.5699782712768963, Accuracy: 0.6980091148956584\n",
      "Epoch 6/6, Validation Loss: 0.5767913465281479, Accuracy: 0.702566562724874\n",
      "Loss: 0.5767913465281479, Accuracy: 0.702566562724874\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.702566562724874"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNAClassifierCNN().to(DEVICE)\n",
    "evaluation_loop(model, epochs=6, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam optimization\n",
    "\n",
    "Let's try to optimize the learning rate, number of training epochs and size of the convolution kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_float('learning_rate', 0.00001, 0.01)\n",
    "    epochs = trial.suggest_int('epochs', 9, 10)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 4, 5)\n",
    "\n",
    "    print(f\"LR: {lr}, Epochs: {epochs}, Kernel size: {kernel_size}\")\n",
    "\n",
    "    model = DNAClassifierCNN(kernel_size=kernel_size).to(DEVICE)\n",
    "\n",
    "    acc = evaluation_loop(model, epochs, lr)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best value (validation AU PRC): {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5823382960820417\n",
      "Test Accuracy: 0.6973229706390328\n"
     ]
    }
   ],
   "source": [
    "# Načítanie testovacích dát\n",
    "test_ds = DNADataset(dataset[\"test\"].with_format(\"torch\"))\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# Nastavenie stratovej funkcie\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Spustenie evaluácie\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "# Výsledky\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "af167c304fdc99884e31a029277e630a5b00036f91292350fffdeed1d15b16ff"
  },
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
